import requests
from bs4 import BeautifulSoup
from selenium import webdriver
import time
import pandas as pd

def extract_blog_data(post):
    title = post.find('h2').text.strip()
    date = post.find('time').text.strip()
    image_url = post.find('img')['src']
    likes_count = int(post.find('span', class_='like-count').text.strip())

    return {
        'Title': title,
        'Date': date,
        'Image URL': image_url,
        'Likes Count': likes_count
    }

def scrape_page(url, use_selenium=False):
    if use_selenium:
        driver = webdriver.Firefox()
        driver.get(url)
        time.sleep(2)
        content = driver.page_source
        driver.quit()
    else:
        response = requests.get(url)
        content = response.content

    soup = BeautifulSoup(content, 'html.parser')
    blog_posts = soup.find_all('article', class_='post-item')

    data = []
    for post in blog_posts:
        blog_data = extract_blog_data(post)
        data.append(blog_data)

    return data

def save_to_csv(data, filename='C:/Users/KISHORE/amit 2/blog_data.csv'):
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    print(f"Data saved to {filename}")

def main():
    base_url = "https://rategain.com/blog"
    total_pages = 5

    all_data = []
    for page in range(1, total_pages + 1):
        page_url = f"{base_url}/page/{page}/"
        print(f"Scraping page {page}...")

        page_data = scrape_page(page_url, use_selenium=(page == 1))
        all_data.extend(page_data)

    save_to_csv(all_data)

if __name__ == "__main__":
    main()
